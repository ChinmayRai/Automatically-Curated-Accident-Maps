We have not published any research paper in any Journal as of yet.

This Project is a Joint Venture of Samarth Aggarwal and Chinmay Rai, both of whom are Junior Undergraduates at the Indian Institute of Technology Delhi. Our Guide for the project is Dr Rijurekha Sen who is Assistant Professor at IIT Delhi. We were also co-advised by Prof. Mausam (IITD), for some of the technicalities of our work. Apart from this some of the Researchers who have helped us are Swarnadeep Saha, Prof.Srikant bedathur Aman Madaan.

The kind of Accident statistic which available in the current times has a very coarse granularity in the sense that it shows the collective data for a Sizeable region. Our aim with his project is to treat each accident as a separate event and pin-point its location on the map. This will help highlight the specific accident-prone location and help us correlate the causes of those accidents, to direct the efforts of the authorities towards addressing those issues. Everyday more than 250 people are killed in India, even if the system is able to point to one of the causes, then it create major impact. Accident events can be analyzed across time and space to correlate them with cycles of week, work-hours and social Schedule to come up with effective counter-measures

Our project does not belong to the typical product category, for which extensive quantity-quality optimizations have to done to bring it to the assmebly line. Instead our Work aims to create Impact on a Larger scale from a single point of Analysis. There is no question of large scale production, because it is an software package and is to be deployed on one to many basis. However the complete development before delivery can not be imagined without changing some particulars of the project. For example some of the librabies/Application programming Interfaces (API) that we have used are open source and not as good as we would like them to be. Some API provided limited access giving rise to scalabilty issue. Thus in order to use dome of the better librabry and ensure scalability of the project we estimate that we would require about Rupees. This amount will mainly be put to use in renting Google cloud services(NLP, Maps) and CartoDB full (Mapping Interface) access.





Our project is extremely frugal in terms of saving lives. It aims to use open source data to try and Highlight Infrastructure issues in Road Traffic, potentially saving multiple lives. The cost of processing thousands of accident cases is a few hundred rupees, which is significantly less than the amount the public authorities use for putting up "Speed Kills" and "Don't drink and drive". Yet our software promises to be more effective than those awareness attempts. Hence we see it as an extremely frugal way of churning free data to potentially save human lives.

The key resources used by our project are Open-source data, Human-Resource (Research work of some people including us) and some paid-software services. The only materialistic resource out of these is the paid-software services because we are getting the data free of cost. Thus our project creates "Knowledge" Value from open-source data using "marginal" resources. We use the word marginal because processing data for a city by investing a few thousand rupees will have an impact of lives of all the people on Roads. This knowledge value can prove to be instrumental in saving precious human Lives. Thus it creates more value from less material resources for the benefit of all. 

The core of this project according to us is processing data. We live in the age of Big Data, where data in explosive in terms of Volume, Variety, and Velocity. This data can be used to solve and analyze many real-life problems. However, it cannot be imagined that this analysis be done by the Humans as the amount of data is huge and we get bored pretty soon. Hence the quest of this age is to make Computers, understand that Data (Machine Learning and Artificial Intelligence). Print and Digital media are the form of storage for most of this data. However the Biggest problem being that most of it, is in Natural Languages and hence requires to be converted into a machine understandable format. The Field of Natural language Processing tries to solve this problem. Our work here is another manifestation of this problem where we try to solve Traffic Issues by training the machines to understand News from Archives. It is important to note that this work can be seen solution in a very broad domain of applicationss if we generalize the model. For example, The same Functional Model can be used to track crime statistics using newspapers, if we tweak the model to some extent. Hence it is intrumental to view this work as a stepping stone in the process of Making the computers understand Natural languages, which in turn, promises applications in Multiple avenues of life.